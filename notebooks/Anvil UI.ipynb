{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e35eaf-9263-4d43-a20d-138793afb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6530b86d-8662-481a-a8ec-a2ea9ad84f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from uuid import uuid4\n",
    "from ultralytics import YOLO\n",
    "import anvil.server\n",
    "import anvil.media\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859abd34-18f4-4942-b951-89d850bb52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = 'temp'\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6840ccce-d073-43e4-99c3-99c0d189ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n",
    "    label_length = ops.cast(ops.squeeze(label_length, axis=-1), dtype=\"int32\")\n",
    "    input_length = ops.cast(ops.squeeze(input_length, axis=-1), dtype=\"int32\")\n",
    "    sparse_labels = ops.cast(\n",
    "        ctc_label_dense_to_sparse(y_true, label_length), dtype=\"int32\"\n",
    "    )\n",
    "\n",
    "    y_pred = ops.log(ops.transpose(y_pred, axes=[1, 0, 2]) + keras.backend.epsilon())\n",
    "\n",
    "    return ops.expand_dims(\n",
    "        tf.compat.v1.nn.ctc_loss(\n",
    "            inputs=y_pred, labels=sparse_labels, sequence_length=input_length\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "\n",
    "\n",
    "def ctc_label_dense_to_sparse(labels, label_lengths):\n",
    "    label_shape = ops.shape(labels)\n",
    "    num_batches_tns = ops.stack([label_shape[0]])\n",
    "    max_num_labels_tns = ops.stack([label_shape[1]])\n",
    "\n",
    "    def range_less_than(old_input, current_input):\n",
    "        return ops.expand_dims(ops.arange(ops.shape(old_input)[1]), 0) < tf.fill(\n",
    "            max_num_labels_tns, current_input\n",
    "        )\n",
    "\n",
    "    init = ops.cast(tf.fill([1, label_shape[1]], 0), dtype=\"bool\")\n",
    "    dense_mask = tf.compat.v1.scan(\n",
    "        range_less_than, label_lengths, initializer=init, parallel_iterations=1\n",
    "    )\n",
    "    dense_mask = dense_mask[:, 0, :]\n",
    "\n",
    "    label_array = ops.reshape(\n",
    "        ops.tile(ops.arange(0, label_shape[1]), num_batches_tns), label_shape\n",
    "    )\n",
    "    label_ind = tf.compat.v1.boolean_mask(label_array, dense_mask)\n",
    "\n",
    "    batch_array = ops.transpose(\n",
    "        ops.reshape(\n",
    "            ops.tile(ops.arange(0, label_shape[0]), max_num_labels_tns),\n",
    "            tf.reverse(label_shape, [0]),\n",
    "        )\n",
    "    )\n",
    "    batch_ind = tf.compat.v1.boolean_mask(batch_array, dense_mask)\n",
    "    indices = ops.transpose(\n",
    "        ops.reshape(ops.concatenate([batch_ind, label_ind], axis=0), [2, -1])\n",
    "    )\n",
    "\n",
    "    vals_sparse = tf.compat.v1.gather_nd(labels, indices)\n",
    "\n",
    "    return tf.SparseTensor(\n",
    "        ops.cast(indices, dtype=\"int64\"),\n",
    "        vals_sparse,\n",
    "        ops.cast(label_shape, dtype=\"int64\"),\n",
    "    )\n",
    "\n",
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = ops.cast(ops.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = ops.cast(ops.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = ops.cast(ops.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * ops.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * ops.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "img_width = 500\n",
    "img_height = 50\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n",
    "    input_shape = ops.shape(y_pred)\n",
    "    num_samples, num_steps = input_shape[0], input_shape[1]\n",
    "    y_pred = ops.log(ops.transpose(y_pred, axes=[1, 0, 2]) + keras.backend.epsilon())\n",
    "    input_length = ops.cast(input_length, dtype=\"int32\")\n",
    "\n",
    "    if greedy:\n",
    "        (decoded, log_prob) = tf.nn.ctc_greedy_decoder(\n",
    "            inputs=y_pred, sequence_length=input_length\n",
    "        )\n",
    "    else:\n",
    "        (decoded, log_prob) = tf.compat.v1.nn.ctc_beam_search_decoder(\n",
    "            inputs=y_pred,\n",
    "            sequence_length=input_length,\n",
    "            beam_width=beam_width,\n",
    "            top_paths=top_paths,\n",
    "        )\n",
    "    decoded_dense = []\n",
    "    for st in decoded:\n",
    "        st = tf.SparseTensor(st.indices, st.values, (num_samples, num_steps))\n",
    "        decoded_dense.append(tf.sparse.to_dense(sp_input=st, default_value=-1))\n",
    "    return (decoded_dense, log_prob)\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "\n",
    "data_dir = Path('../data/processed/sudoku_ocr_dataset/')\n",
    "\n",
    "# Get list of all the images\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\".jpg\")[0].split('_')[0] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "max_length = max([len(label) for label in labels])\n",
    "\n",
    "# Mapping characters to integers\n",
    "char_to_num = layers.StringLookup(vocabulary=list(characters), mask_token=None)\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bb4ae4-5dd3-491a-902a-ed3c9bc87f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shbnd\\anaconda3\\envs\\dev\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shbnd\\AppData\\Local\\Temp\\ipykernel_18636\\3441669525.py:11: The name tf.nn.ctc_loss is deprecated. Please use tf.compat.v1.nn.ctc_loss instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "localization_model = YOLO(r\"C:\\Users\\shbnd\\Desktop\\Work\\sudoku-solver\\models\\localization_yolo_model\\weights\\best.pt\")\n",
    "model = keras.models.load_model('../models/sudoku_ocr.keras', custom_objects={'CTCLayer': CTCLayer})\n",
    "prediction_model = keras.models.Model(\n",
    "    model.input[0], model.get_layer(name=\"dense2\").output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f859f9-8e3d-49bd-bb4f-95a9faeafef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Default environment\" as SERVER\n",
      "\n",
      "image 1/1 C:\\Users\\shbnd\\Desktop\\Work\\sudoku-solver\\notebooks\\temp\\5fd49cb4-1a6a-4eb1-af4b-9c094f990415\\WhatsApp Image 2025-02-01 at 21.59.13_3036ea17.jpg: 640x416 1 puzzle, 278.5ms\n",
      "Speed: 9.1ms preprocess, 278.5ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\n",
      "image 1/1 C:\\Users\\shbnd\\Desktop\\Work\\sudoku-solver\\notebooks\\temp\\2852b4ea-0c6f-4958-ada3-42bb3f7000ec\\WhatsApp Image 2025-01-31 at 02.14.39_7e73e84a.jpg: 640x288 1 puzzle, 200.2ms\n",
      "Speed: 4.4ms preprocess, 200.2ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\n",
      "image 1/1 C:\\Users\\shbnd\\Desktop\\Work\\sudoku-solver\\notebooks\\temp\\be97f5ec-e1cd-409b-a850-a4c22fb5858c\\WhatsApp Image 2025-02-01 at 21.59.13_3036ea17.jpg: 640x416 1 puzzle, 196.1ms\n",
      "Speed: 4.5ms preprocess, 196.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\n",
      "image 1/1 C:\\Users\\shbnd\\Desktop\\Work\\sudoku-solver\\notebooks\\temp\\e9d12ccb-3f03-46d9-bf59-6cf93aea309f\\WhatsApp Image 2025-01-31 at 02.14.39_7e73e84a.jpg: 640x288 1 puzzle, 129.1ms\n",
      "Speed: 4.0ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\n",
      "image 1/1 C:\\Users\\shbnd\\Desktop\\Work\\sudoku-solver\\notebooks\\temp\\86aeecaa-784b-4fd6-87ae-96af272772ca\\WhatsApp Image 2025-02-01 at 21.59.13_3036ea17.jpg: 640x416 1 puzzle, 184.5ms\n",
      "Speed: 0.0ms preprocess, 184.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Anvil websocket closed (code 1006, reason=Going away)\n",
      "Reconnecting Anvil Uplink...\n",
      "Connecting to wss://anvil.works/uplink\n",
      "Reconnection failed. Waiting 10 seconds, then retrying.\n",
      "Reconnecting Anvil Uplink...\n",
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Default environment\" as SERVER\n"
     ]
    }
   ],
   "source": [
    "anvil.server.connect(\"BTYQVFMCXQHW2WSJLIZTKPBI-JYI3TDO2QSP3VVRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f30858-4539-4b7c-a87f-1a29074a27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def download_image(file):\n",
    "    uuid_string = str(uuid4())\n",
    "    request_dir = os.path.join(temp_dir, uuid_string)\n",
    "    os.makedirs(request_dir, exist_ok=True)\n",
    "    image_path = os.path.join(request_dir, file.name)\n",
    "    fh = open(image_path, 'wb')\n",
    "    fh.write(file.get_bytes())\n",
    "    fh.close()\n",
    "\n",
    "    result = localization_model(image_path)\n",
    "    result_json = json.loads(result[0].to_json())\n",
    "    bbox_dict = result_json[0]['box']\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    image = image[int(bbox_dict['y1']): int(bbox_dict['y2']), int(bbox_dict['x1']): int(bbox_dict['x2'])]\n",
    "    \n",
    "    H, W, _ = image.shape\n",
    "    \n",
    "    h = int(H/9)\n",
    "    w = int(W/9)\n",
    "\n",
    "    result = {}\n",
    "    for idx in range(9):\n",
    "        file_path = os.path.join(request_dir, f'image_{idx}.jpg')\n",
    "        row_image = image[idx*h: (idx+1)*h, :]\n",
    "        cv2.imwrite(file_path, row_image)\n",
    "        result[f'image_{idx+1}'] = anvil.media.from_file(file_path)\n",
    "        \n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.io.decode_png(img, channels=1)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = ops.image.resize(img, [img_height, img_width])\n",
    "        img = ops.transpose(img, axes=[1, 0, 2])\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        preds = prediction_model.predict(img)\n",
    "        pred_texts = decode_batch_predictions(preds)\n",
    "        result[f'result_{idx+1}'] = pred_texts[0]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb57fb-7550-4e9a-9c9c-2aa2e2236288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ad2231-cdf8-42c0-9553-4557b118ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d7729-3e44-4129-a2a6-c423d6e0431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = ''\n",
    "\n",
    "output_string = solve(input_string) # v1\n",
    "output_string, solution_dict = solve(input_string) # v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09066549-6f1d-44b0-b9b7-379a890f55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_dict = {\n",
    "    'pos_idx': sol_idx\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
